{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework will be dedicated to Keyword Spotting (KWS), streaming and speedup NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, nine, no, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "datadir = \"speech_commands\"\n",
    "\n",
    "#!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
    "#!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
    "\n",
    "samples_by_target = {\n",
    "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
    "    for cls in os.listdir(datadir)\n",
    "    if os.path.isdir(os.path.join(datadir, cls))\n",
    "}\n",
    "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from IPython import display as display_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = pd.DataFrame(columns=['name', 'word', 'label'])\n",
    "#for el in tqdm(samples_by_target.keys()):\n",
    "#    if el != '_background_noise_':\n",
    "#        for name in samples_by_target[el]:\n",
    "#            word = name.split('/')[1]\n",
    "#            if word == 'sheila':\n",
    "#                label = 1\n",
    "#            elif word == 'marvin':\n",
    "#                label = 2\n",
    "#            else:\n",
    "#                label = 0\n",
    "#            labels = labels.append({'name':name, 'word':word, 'label':label}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "БЭКГРАУНД ЗВУКИ В АУГМЕНТАЦИЮ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background_noises = pd.DataFrame(columns=['name'])\n",
    "\n",
    "#for el in tqdm(samples_by_target.keys()):\n",
    "#    if el == '_background_noise_':\n",
    "#        for name in samples_by_target[el]:\n",
    "#            if 'README' not in name:\n",
    "#                background_noises = background_noises.append(\n",
    "#                    {'name':name}, ignore_index=True\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background_noises.to_csv('background_noises.csv', index=False)\n",
    "\n",
    "#background_noises = background_noises['name'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Choose from 1 to 3 keywords to your liking, and use the rest as negative examples.\n",
    "    We recommend to use sheila and/or marvin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rand_noise(audio):\n",
    "    background_noises = [\n",
    "        'speech_commands/_background_noise_/white_noise.wav',\n",
    "       'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "       'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "       'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "       'speech_commands/_background_noise_/pink_noise.wav',\n",
    "       'speech_commands/_background_noise_/running_tap.wav'\n",
    "    ]\n",
    "    \n",
    "    noise_num = torch.randint(low=0, high=len(background_noises), size=(1,)).item()    \n",
    "    noise = torchaudio.load(background_noises[noise_num])[0].squeeze()    \n",
    "    \n",
    "    noize_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "    noize_energy = torch.norm(noise)\n",
    "    audio_energy = torch.norm(audio)\n",
    "\n",
    "    alpha = (audio_energy / noize_energy) * torch.pow(10, -noize_level / 20)\n",
    "\n",
    "    start = torch.randint(low=0, high=int(noise.size(0) - audio.size(0) - 1), size=(1,)).item()\n",
    "    noise_sample = noise[start : start + audio.shape[0]]\n",
    "\n",
    "    audio_new = audio + alpha * noise_sample\n",
    "    audio_new.clamp_(-1, 1)\n",
    "    \n",
    "    return audio_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this homework assignment, you will need to implement a model for finding a keyword in a stream.\n",
    "\n",
    "        1) https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf\n",
    "            It is recommended to implement the version with CNN because it is easy and improves the model.\n",
    "\n",
    "        2) What about streaming?\n",
    "           This model works differently during training and inferance.\n",
    "           During training you have some fixed input and you know that it has a keyword (or not).\n",
    "           During the inferance, you read the T frames and make a prediction on them. And the next step is to read the T+1 frame,\n",
    "           run the neural network just for it, and make a prediction based on it and the T-1 of the previous frames.\n",
    "           This way you don't make unnecessary calculations.\n",
    "\n",
    "           So, your model should support streaming mode.\n",
    "           To demonstrate the work in streaming mode, take two random audio tracks of 10-20 seconds and glue them together\n",
    "           so that your keyword will be between them. Run the model through this glued track and draw how the probability of your keyword changing over time.\n",
    "\n",
    "        3) A good KWS is a robust KWS, so we ask you to implement as many augmentations as possible.\n",
    "           (bonus) Download any noise from YouTube and add it as a background noise to the positive data. This helps a lot in real life.\n",
    "           P.S. Use https://www.youtube-dl.org/\n",
    "\n",
    "        4) (bonus) Add more attentions and orthogonality regularization. https://arxiv.org/abs/1910.04500\n",
    "        \n",
    "        5) (bonus) Speedup you model! Implement distillation of your model, for example,\n",
    "            train the LSTM with 256 hidden size and distil it into LSTM with 128 hidden size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) In this homework you are allowed to use pytorch-lighting.\n",
    "\n",
    "    2) Try to write code more structurally and cleanly!\n",
    "    \n",
    "    3) Good logging of experiments save your nerves and time, so we ask you to use W&B. Log loss, FA/FR rate or something else.\n",
    "        Do not remove the logs until we have checked your work and given you a grade!\n",
    "    \n",
    "    4) (Bonus) We also ask you to organize your code in github repo with Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n",
    "    \n",
    "    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n",
    "    \n",
    "    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some audio track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 35\n",
    "N_MELS     = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom competition dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root='', csv_path='labels.csv', kw='sheila', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.kw = kw\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_name = self.root + self.csv.loc[idx, 'name']\n",
    "        utt = torchaudio.load(utt_name)[0].squeeze()\n",
    "        word = self.csv.loc[idx, 'word']\n",
    "        label = self.csv.loc[idx, 'label']\n",
    "        \n",
    "        if self.transform:\n",
    "            utt = self.transform(utt)\n",
    "\n",
    "        sample = {'utt': utt, 'word': word, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментации и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tr(wav):\n",
    "    aug_num = torch.randint(low=0, high=4, size=(1,)).item()\n",
    "    augs = [\n",
    "        lambda x: x,\n",
    "        lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "        lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "        lambda x: add_rand_noise(x)\n",
    "    ]\n",
    "    \n",
    "    return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(wav, label):\n",
    "    print(label)\n",
    "    display_.display(display_.Audio(wav, rate=48000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate_fn и Mel'ы и датасеты\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за этого ауги накладываются на валидацию тоже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all train+val samples: 64721\n"
     ]
    }
   ],
   "source": [
    "my_dataset = TrainDataset(csv_path='labels.csv', transform=transform_tr)\n",
    "print('all train+val samples:', len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = np.random.randint(0, 64721, 120)\n",
    "#my_dataset = torch.utils.data.Subset(my_dataset, ind)\n",
    "\n",
    "train_len = 57500\n",
    "val_len = 64721 - train_len \n",
    "# train_len = 100\n",
    "# val_len = 20\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampler(target):\n",
    "    \n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big\n",
    "train_sampler = get_sampler(train_set.dataset.csv['label'][train_set.indices].values)\n",
    "val_sampler   = get_sampler(val_set.dataset.csv['label'][val_set.indices].values)\n",
    "\n",
    "# small\n",
    "# train_sampler = get_sampler(train_set.dataset.dataset.csv['label'][train_set.indices].values)\n",
    "#val_sampler   = get_sampler(val_set.dataset.dataset.csv['label'][val_set.indices].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    wavs = []\n",
    "    labels = []    \n",
    "        \n",
    "    for el in data:\n",
    "        wavs.append(el['utt'])\n",
    "        labels.append(el['label'])\n",
    "\n",
    "    wavs = pad_sequence(wavs, batch_first=True)\n",
    "    labels = torch.Tensor(labels).type(torch.long)\n",
    "    \n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading data and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=preprocess_data, \n",
    "                          sampler=train_sampler, drop_last=False,\n",
    "                          num_workers=1, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=preprocess_data, \n",
    "                        sampler=val_sampler, drop_last=False,\n",
    "                        num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations\n",
    "\n",
    "#n_fft=1024, hop_length=256,\n",
    "melspec_train = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000,  n_mels=N_MELS),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# no augmentations\n",
    "melspec_val = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=48000,           ### 22050, 48000\n",
    "    # n_fft=1024,\n",
    "    # hop_length=256,\n",
    "    n_mels=N_MELS\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FA - 0, выдал 1\n",
    "\n",
    "FR - 1, выдал 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_FA_FR(preds, labels):\n",
    "    \n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    return FA.item()/torch.numel(preds), FR.item()/torch.numel(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть спека размером длина аудио?\n",
    "\n",
    "-> (BS, 40, 41), где 40 - это высота (n_mels), а 41 - 30 прошлое, 1 сейчас и 10 будущее "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из статьи CRNN:**\n",
    "\n",
    "Работали с 40 x 151.\n",
    "\n",
    "BS = 64\n",
    "\n",
    "LR = 0.001 -> 0.0003\n",
    "\n",
    "**Как вариант можно взять:**\n",
    "\n",
    "out_channels = 16\n",
    "\n",
    "GRU num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accordingly, in the atten-\n",
    "tion models, the input window has set to 189 frames to cover\n",
    "the length of the wake-up word.\n",
    "\n",
    "At runtime, the sliding window was set to 100\n",
    "frames and frame shift was set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepconv(in_size, out_size, kernel_size, stride=1, dilation=1, padding=0):\n",
    "    #if padding is None:\n",
    "    #    padding = (kernel_size-1)//2\n",
    "    return nn.Sequential(\n",
    "        torch.nn.Conv1d(in_size, in_size, kernel_size[1], \n",
    "                        stride=stride[1], dilation=dilation, groups=in_size,\n",
    "                        padding=padding),\n",
    "        \n",
    "        torch.nn.Conv1d(in_size, out_size, kernel_size=1, \n",
    "                        stride=stride[0], groups=int(in_size/kernel_size[0])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "          \n",
    "        self.sepconv = sepconv(in_size=40, out_size=128, kernel_size=(20, 5), stride=(8, 2))\n",
    "        self.gru = nn.GRU(input_size=128, hidden_size=128, num_layers=2, dropout=0.1, bidirectional=True)\n",
    "        #self.linear = nn.Linear(in_features=128, out_features=64)   # HS * num_directions\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.sepconv(x)\n",
    "        \n",
    "        # (BS, HS, ?) -> (HS, BS, ?) ->(seq_len, BS, HS)\n",
    "        x = x.transpose(0, 1).transpose(0, 2)\n",
    "        \n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        # x : (seq_len, BS, HS * num_dirs)\n",
    "        # hidden : (num_layers * num_dirs, BS, HS)\n",
    "                \n",
    "        #x = self.linear(x)\n",
    "                        \n",
    "        return x, hidden ###   #  DIM ?? ? ? ? добавить софтмакс?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnMech(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttnMech, self).__init__()\n",
    "        \n",
    "        self.Wx_b = nn.Linear(256, 256)   #   ???\n",
    "        self.Vt   = nn.Linear(256, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.Wx_b(x))\n",
    "        e = self.Vt(x)\n",
    "\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LastLayer, self).__init__()\n",
    "        \n",
    "        self.U = nn.Linear(256, 3, bias=False)     # ???\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, e, data):\n",
    "        \n",
    "        data = data.transpose(0, 1)    # (BS, seq_len, hid_size*num_dirs)\n",
    "                             \n",
    "        a = F.softmax(e, dim=-1).unsqueeze(1)                # ???\n",
    "        \n",
    "        c = torch.bmm(a, data).squeeze()\n",
    "        # should be c.size() == GRU_hidden_size\n",
    "        \n",
    "        Uc = self.U(c)\n",
    "        \n",
    "        probs = F.log_softmax(Uc, dim=-1)     # log???\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastLayer(\n",
       "  (U): Linear(in_features=256, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRNN_model = CRNN()\n",
    "attn_layer = AttnMech()\n",
    "last_layer = LastLayer()\n",
    "\n",
    "CRNN_model.to(device)\n",
    "attn_layer.to(device)\n",
    "last_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('crnn_3class', map_location=device)\n",
    "CRNN_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load('attn_3class', map_location=device)\n",
    "attn_layer.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load('last_layer_3class', map_location=device)\n",
    "last_layer.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564336"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(CRNN_model) +count_parameters(attn_layer) + count_parameters(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkirili4ik\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">whole-sound-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kirili4ik/dla-ht3\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kirili4ik/dla-ht3/runs/2jrjg9fz\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht3/runs/2jrjg9fz</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201103_235316-2jrjg9fz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fe53f828fd0>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.watch(CRNN_model)\n",
    "wandb.watch(attn_layer)\n",
    "wandb.watch(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_crnn = torch.optim.Adam(CRNN_model.parameters(), weight_decay=1e-5)\n",
    "opt_attn = torch.optim.Adam(attn_layer.parameters(), weight_decay=1e-5)\n",
    "opt_last = torch.optim.Adam(last_layer.parameters(), weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [01:54,  1.96it/s]\n",
      "29it [00:14,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "225it [01:48,  2.08it/s]\n",
      "29it [00:13,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "225it [01:41,  2.22it/s]\n",
      "29it [00:13,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "175it [01:18,  2.15it/s]"
     ]
    }
   ],
   "source": [
    "# 1 epoch\n",
    "NUM_EPOCHS = 40\n",
    "for n in range(NUM_EPOCHS):\n",
    "    CRNN_model.train()\n",
    "    attn_layer.train()\n",
    "    last_layer.train()\n",
    "    for i, el in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        batch = el[0]\n",
    "        labels = el[1]\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "        batch = torch.log(melspec_train(batch) + 1e-9).to(device)   # .batch.narrow(-1, 0, 41)    \n",
    "\n",
    "        opt_crnn.zero_grad()\n",
    "        opt_attn.zero_grad()\n",
    "        opt_last.zero_grad()\n",
    "\n",
    "\n",
    "        hidden = torch.zeros(2*2, batch.size(0), 128).to(device)    # (num_layers*num_dirs,  BS, HS)\n",
    "        e = []\n",
    "\n",
    "        output, hidden = CRNN_model(batch, hidden)\n",
    "        # output: (seq_len, BS, hidden*num_dir)  or just hidden?\n",
    "\n",
    "        for el in output:\n",
    "            e_t = attn_layer(el)       # (BS, 1)        # OR HIDDEN ???\n",
    "            e.append(e_t)\n",
    "\n",
    "        e = torch.cat(e, dim=1)   # (BS, seq_len)\n",
    "\n",
    "        probs = last_layer(e, output)\n",
    "\n",
    "        loss = F.nll_loss(probs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(CRNN_model.parameters(), 10)\n",
    "        torch.nn.utils.clip_grad_norm_(attn_layer.parameters(), 10)\n",
    "        torch.nn.utils.clip_grad_norm_(last_layer.parameters(), 10)\n",
    "\n",
    "        opt_crnn.step()\n",
    "        opt_attn.step()\n",
    "        opt_last.step()\n",
    "\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        acc = torch.true_divide(\n",
    "                            torch.sum(argmax_probs == labels), \n",
    "                            torch.numel(argmax_probs)\n",
    "        )\n",
    "        wandb.log({'loss':loss.item(), 'train_FA':FA, 'train_FR':FR, 'train_acc':acc})\n",
    "        \n",
    "        \n",
    "        \n",
    "    # VALIDATION\n",
    "    CRNN_model.eval()\n",
    "    attn_layer.eval()\n",
    "    last_layer.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        accs = []\n",
    "        FAs, FRs = [], []\n",
    "        for i, el in tqdm(enumerate(val_loader)):\n",
    "            batch = el[0]\n",
    "            labels = el[1]\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "            batch = torch.log(melspec_val(batch) + 1e-9).to(device)  \n",
    "\n",
    "            hidden = torch.zeros(2*2, batch.size(0), 128).to(device)    # (num_layers*num_dirs,  BS, HS)\n",
    "            e = []\n",
    "            \n",
    "            output, hidden = CRNN_model(batch, hidden)\n",
    "            # output: (seq_len, BS, hidden*num_dir)  or just hidden?\n",
    "\n",
    "            for el in output:\n",
    "                e_t = attn_layer(el)       # (BS, 1)        # OR HIDDEN ???\n",
    "                e.append(e_t)\n",
    "\n",
    "            e = torch.cat(e, dim=1)   # (BS, seq_len)\n",
    "\n",
    "            probs = last_layer(e, output)\n",
    "\n",
    "            loss = F.nll_loss(probs, labels)\n",
    "            \n",
    "            argmax_probs = torch.argmax(probs, dim=-1)\n",
    "            FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            accs.append(torch.true_divide(\n",
    "                                torch.sum(argmax_probs == labels), \n",
    "                                torch.numel(argmax_probs)).item()\n",
    "                       )\n",
    "            FAs.append(FA)\n",
    "            FRs.append(FR)\n",
    "            \n",
    "        wandb.log({'mean_val_loss':np.mean(val_losses), 'mean_val_acc':np.mean(accs),\n",
    "                   'mean_val_FA':np.mean(FAs), 'mean_val_FR':np.mean(FRs)}) \n",
    "    \n",
    "    if n > 0 and n % 15 == 0:\n",
    "        opt_crnn.param_groups[0]['lr'] = 0.005\n",
    "        opt_attn.param_groups[0]['lr'] = 0.005\n",
    "        opt_last.param_groups[0]['lr'] = 0.005\n",
    "\n",
    "    \n",
    "    print('END OF EPOCH', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''torch.save({\n",
    "    'model_state_dict': CRNN_model.state_dict(),\n",
    "}, 'crnn_3class')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': attn_layer.state_dict(),\n",
    "}, 'attn_3class')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': last_layer.state_dict(),\n",
    "}, 'last_layer_3class')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8103dc777a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_sheila_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d42dd7235689>\u001b[0m in \u001b[0;36mviz\u001b[0;34m(wav, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdisplay_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_make_wav\u001b[0;34m(data, rate, normalize)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_with_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_without_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_validate_and_normalize_with_numpy\u001b[0;34m(data, normalize)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "test_audio = torchaudio.load('test_sheila_2')[0].squeeze()\n",
    "viz(test_audio, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2458])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(melspec_val(test_audio.to(device)) + 1e-9).to(device).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2458"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#win_len=512, hop_len=256\n",
    "int((test_audio.size(0) - 400)/200) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([3, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "all_probs = []\n",
    "\n",
    "CRNN_model.eval()\n",
    "attn_layer.eval()\n",
    "last_layer.eval()\n",
    "with torch.no_grad():\n",
    "    start = 41\n",
    "    finish = int((test_audio.size(0) - 400)/200) + 3                                   #  int((test_audio.size(0) - win_len)/hop_len) : 400, 200???\n",
    "\n",
    "    test_audio_mel = torch.log(melspec_val(test_audio) + 1e-9).unsqueeze(0).to(device)\n",
    "    hidden = torch.zeros(2*2, 1, 128).to(device)    # (num_layers*num_dirs, BS, HS)\n",
    "        \n",
    "    e = []\n",
    "    outputs, hidden = CRNN_model(test_audio_mel[:, :, 0 : start], hidden)\n",
    "    for el in outputs:\n",
    "        e_t = attn_layer(el)\n",
    "        e.append(e_t)\n",
    "    new_e = torch.cat(e, dim=1)\n",
    "    probs = last_layer(new_e, outputs)\n",
    "    all_probs.append(torch.exp(probs[1]))\n",
    "        \n",
    "    end = (finish - start + 1)\n",
    "    start -= 5\n",
    "    for i in range(5, end, 5):\n",
    "        e = e[1:]   # delete first\n",
    "        outputs = outputs[1:]    \n",
    "                        \n",
    "        batch_now = test_audio_mel[:, :, start + i : start + i + 5]\n",
    "        \n",
    "        output, hidden = CRNN_model(batch_now, hidden)   # hidden is also new!\n",
    "        # output: (1, BS, hidden*num_dir)  or just hidden?\n",
    "        outputs = torch.cat([outputs, output])        \n",
    "        \n",
    "        e_t = attn_layer(output.squeeze(0))\n",
    "                \n",
    "        e.append(e_t)\n",
    "        \n",
    "        new_e = torch.cat(e, dim=1)\n",
    "        probs = last_layer(new_e, outputs)\n",
    "        all_probs.append(torch.exp(probs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5327150450>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('time frame')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.plot(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ИГРЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 40, 41))   # картинка 40 x 41\n",
    "a = a.unsqueeze(1)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (20, 5)\n",
    "stride = (8, 2)\n",
    "\n",
    "in_size = 1\n",
    "out_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_size, out_size, kernel_size=(20, 5), stride=(8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 3, 19])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(a).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEPARABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_w = nn.Conv2d(in_size, in_size, kernel_size, \n",
    "                             stride=stride, groups=in_size)\n",
    "point_w = nn.Conv2d(in_size, out_size, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 3, 19])\n",
      "torch.Size([3, 32, 3, 19])\n"
     ]
    }
   ],
   "source": [
    "depth_res = depth_w(a)\n",
    "print(depth_res.size())\n",
    "\n",
    "res = point_w(depth_res)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3232"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(depth_w) + count_parameters(point_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ОНО???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 41])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 40, 41))   # картинка 40 x 41\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бегаем вправо-влево по картинке 40x41 окошком 5x1, таких окошек 40, для каждой строчки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv1d(40, 40, kernel_size=5, groups=40, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40, 19])\n"
     ]
    }
   ],
   "source": [
    "res1 = layer(a)\n",
    "print(res1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бегаем вправо-влево по картинке 40x? окошком 20x1, а 20 берется из 40/groups=20. При этом глубину выставляем 32, это все предыдущее предложение просто проделывается 32 раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = nn.Conv1d(40, 32, kernel_size=1, groups=2, stride=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "res2 = layer2(res1)\n",
    "print(res2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КОНЕЦ ИГР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
