{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework will be dedicated to Keyword Spotting (KWS), streaming and speedup NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, nine, no, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "datadir = \"speech_commands\"\n",
    "\n",
    "#!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
    "#!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
    "\n",
    "samples_by_target = {\n",
    "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
    "    for cls in os.listdir(datadir)\n",
    "    if os.path.isdir(os.path.join(datadir, cls))\n",
    "}\n",
    "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from IPython import display as display_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = pd.DataFrame(columns=['name', 'word', 'label'])\n",
    "#for el in tqdm(samples_by_target.keys()):\n",
    "#    if el != '_background_noise_':\n",
    "#        for name in samples_by_target[el]:\n",
    "#            word = name.split('/')[1]\n",
    "#            if word == 'sheila':\n",
    "#                label = 1\n",
    "#            else:\n",
    "#                label = 0\n",
    "#            labels = labels.append({'name':name, 'word':word, 'label':label}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "БЭКГРАУНД ЗВУКИ В АУГМЕНТАЦИЮ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background_noises = pd.DataFrame(columns=['name'])\n",
    "\n",
    "#for el in tqdm(samples_by_target.keys()):\n",
    "#    if el == '_background_noise_':\n",
    "#        for name in samples_by_target[el]:\n",
    "#            if 'README' not in name:\n",
    "#                background_noises = background_noises.append(\n",
    "#                    {'name':name}, ignore_index=True\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background_noises.to_csv('background_noises.csv', index=False)\n",
    "\n",
    "#background_noises = background_noises['name'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Choose from 1 to 3 keywords to your liking, and use the rest as negative examples.\n",
    "    We recommend to use sheila and/or marvin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rand_noise(audio):\n",
    "    background_noises = [\n",
    "        'speech_commands/_background_noise_/white_noise.wav',\n",
    "       'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "       'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "       'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "       'speech_commands/_background_noise_/pink_noise.wav',\n",
    "       'speech_commands/_background_noise_/running_tap.wav'\n",
    "    ]\n",
    "    \n",
    "    noise_num = torch.randint(low=0, high=len(background_noises), size=(1,)).item()    \n",
    "    noise = torchaudio.load(background_noises[noise_num])[0].squeeze()    \n",
    "    \n",
    "    noize_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "    noize_energy = torch.norm(noise)\n",
    "    audio_energy = torch.norm(audio)\n",
    "\n",
    "    alpha = (audio_energy / noize_energy) * torch.pow(10, -noize_level / 20)\n",
    "\n",
    "    start = torch.randint(low=0, high=int(noise.size(0) - audio.size(0) - 1), size=(1,)).item()\n",
    "    noise_sample = noise[start : start + audio.shape[0]]\n",
    "\n",
    "    audio_new = audio + alpha * noise_sample\n",
    "    audio_new.clamp_(-1, 1)\n",
    "    \n",
    "    return audio_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this homework assignment, you will need to implement a model for finding a keyword in a stream.\n",
    "\n",
    "        1) https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf\n",
    "            It is recommended to implement the version with CNN because it is easy and improves the model.\n",
    "\n",
    "        2) What about streaming?\n",
    "           This model works differently during training and inferance.\n",
    "           During training you have some fixed input and you know that it has a keyword (or not).\n",
    "           During the inferance, you read the T frames and make a prediction on them. And the next step is to read the T+1 frame,\n",
    "           run the neural network just for it, and make a prediction based on it and the T-1 of the previous frames.\n",
    "           This way you don't make unnecessary calculations.\n",
    "\n",
    "           So, your model should support streaming mode.\n",
    "           To demonstrate the work in streaming mode, take two random audio tracks of 10-20 seconds and glue them together\n",
    "           so that your keyword will be between them. Run the model through this glued track and draw how the probability of your keyword changing over time.\n",
    "\n",
    "        3) A good KWS is a robust KWS, so we ask you to implement as many augmentations as possible.\n",
    "           (bonus) Download any noise from YouTube and add it as a background noise to the positive data. This helps a lot in real life.\n",
    "           P.S. Use https://www.youtube-dl.org/\n",
    "\n",
    "        4) (bonus) Add more attentions and orthogonality regularization. https://arxiv.org/abs/1910.04500\n",
    "        \n",
    "        5) (bonus) Speedup you model! Implement distillation of your model, for example,\n",
    "            train the LSTM with 256 hidden size and distil it into LSTM with 128 hidden size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) In this homework you are allowed to use pytorch-lighting.\n",
    "\n",
    "    2) Try to write code more structurally and cleanly!\n",
    "    \n",
    "    3) Good logging of experiments save your nerves and time, so we ask you to use W&B. Log loss, FA/FR rate or something else.\n",
    "        Do not remove the logs until we have checked your work and given you a grade!\n",
    "    \n",
    "    4) (Bonus) We also ask you to organize your code in github repo with Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n",
    "    \n",
    "    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n",
    "    \n",
    "    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some audio track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 35\n",
    "N_MELS     = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom competition dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root='', csv_path='labels.csv', kw='sheila', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.kw = kw\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_name = self.root + self.csv.loc[idx, 'name']\n",
    "        utt = torchaudio.load(utt_name)[0].squeeze()\n",
    "        word = self.csv.loc[idx, 'word']\n",
    "        label = self.csv.loc[idx, 'label']\n",
    "        \n",
    "        if self.transform:\n",
    "            utt = self.transform(utt)\n",
    "\n",
    "        sample = {'utt': utt, 'word': word, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментации и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tr(wav):\n",
    "    aug_num = torch.randint(low=0, high=4, size=(1,)).item()\n",
    "    augs = [\n",
    "        lambda x: x,\n",
    "        lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "        lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "        lambda x: add_rand_noise(x)\n",
    "    ]\n",
    "    \n",
    "    return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(wav, label):\n",
    "    print(label)\n",
    "    display_.display(display_.Audio(wav, rate=16000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate_fn и Mel'ы и датасеты\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за этого ауги накладываются на валидацию тоже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all train+val samples: 64721\n"
     ]
    }
   ],
   "source": [
    "my_dataset = TrainDataset(csv_path='labels.csv', transform=transform_tr)\n",
    "print('all train+val samples:', len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = np.random.randint(0, 64721, 120)\n",
    "#my_dataset = torch.utils.data.Subset(my_dataset, ind)\n",
    "\n",
    "train_len = 57500\n",
    "val_len = 64721 - train_len \n",
    "# train_len = 100\n",
    "# val_len = 20\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampler(target):\n",
    "    \n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big\n",
    "train_sampler = get_sampler(train_set.dataset.csv['label'][train_set.indices].values)\n",
    "val_sampler   = get_sampler(val_set.dataset.csv['label'][val_set.indices].values)\n",
    "\n",
    "# small\n",
    "# train_sampler = get_sampler(train_set.dataset.dataset.csv['label'][train_set.indices].values)\n",
    "#val_sampler   = get_sampler(val_set.dataset.dataset.csv['label'][val_set.indices].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    wavs = []\n",
    "    labels = []    \n",
    "        \n",
    "    for el in data:\n",
    "        wavs.append(el['utt'])\n",
    "        labels.append(el['label'])\n",
    "\n",
    "    wavs = pad_sequence(wavs, batch_first=True)\n",
    "    labels = torch.Tensor(labels).type(torch.long)\n",
    "    \n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading data and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=preprocess_data, \n",
    "                          sampler=train_sampler, drop_last=False,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=preprocess_data, \n",
    "                        sampler=val_sampler, drop_last=False,\n",
    "                        num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations\n",
    "\n",
    "#n_fft=1024, hop_length=256,\n",
    "melspec_train = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000,  n_mels=N_MELS),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# no augmentations\n",
    "melspec_val = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000,           ### 22050, 48000\n",
    "    # n_fft=1024,\n",
    "    # hop_length=256,\n",
    "    n_mels=N_MELS\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FA - 0, выдал 1\n",
    "\n",
    "FR - 1, выдал 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_FA_FR(preds, labels):\n",
    "    \n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    return FA.item()/torch.numel(preds), FR.item()/torch.numel(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть спека размером длина аудио?\n",
    "\n",
    "-> (BS, 40, 41), где 40 - это высота (n_mels), а 41 - 30 прошлое, 1 сейчас и 10 будущее "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из статьи CRNN:**\n",
    "\n",
    "Работали с 40 x 151.\n",
    "\n",
    "BS = 64\n",
    "\n",
    "LR = 0.001 -> 0.0003\n",
    "\n",
    "**Как вариант можно взять:**\n",
    "\n",
    "out_channels = 16\n",
    "\n",
    "GRU num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accordingly, in the atten-\n",
    "tion models, the input window has set to 189 frames to cover\n",
    "the length of the wake-up word.\n",
    "\n",
    "At runtime, the sliding window was set to 100\n",
    "frames and frame shift was set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepconv(in_size, out_size, kernel_size, stride=1, dilation=1, padding=0):\n",
    "    #if padding is None:\n",
    "    #    padding = (kernel_size-1)//2\n",
    "    return nn.Sequential(\n",
    "        torch.nn.Conv1d(in_size, in_size, kernel_size[1], \n",
    "                        stride=stride[1], dilation=dilation, groups=in_size,\n",
    "                        padding=padding),\n",
    "        \n",
    "        torch.nn.Conv1d(in_size, out_size, kernel_size=1, \n",
    "                        stride=stride[0], groups=int(in_size/kernel_size[0])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "          \n",
    "        self.sepconv = sepconv(in_size=40, out_size=64, kernel_size=(20, 5), stride=(8, 2))\n",
    "        self.gru = nn.GRU(input_size=64, hidden_size=64, num_layers=2, dropout=0.1, bidirectional=True)\n",
    "        #self.linear = nn.Linear(in_features=128, out_features=64)   # HS * num_directions\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.sepconv(x)\n",
    "        \n",
    "        # (BS, HS, ?) -> (HS, BS, ?) ->(seq_len, BS, HS)\n",
    "        x = x.transpose(0, 1).transpose(0, 2)\n",
    "        \n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        # x : (seq_len, BS, HS * num_dirs)\n",
    "        # hidden : (num_layers * num_dirs, BS, HS)\n",
    "                \n",
    "        #x = self.linear(x)\n",
    "                        \n",
    "        return x, hidden ###   #  DIM ?? ? ? ? добавить софтмакс?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnMech(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttnMech, self).__init__()\n",
    "        \n",
    "        self.Wx_b = nn.Linear(128, 128)   #   ???\n",
    "        self.Vt   = nn.Linear(128, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.Wx_b(x))\n",
    "        e = self.Vt(x)\n",
    "\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LastLayer, self).__init__()\n",
    "        \n",
    "        self.U = nn.Linear(128, 2, bias=False)     # ???\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, e, data):\n",
    "        \n",
    "        data = data.transpose(0, 1)    # (BS, seq_len, hid_size*num_dirs)\n",
    "                             \n",
    "        a = F.softmax(e, dim=-1).unsqueeze(1)                # ???\n",
    "        \n",
    "        c = torch.bmm(a, data).squeeze()\n",
    "        # should be c.size() == GRU_hidden_size\n",
    "        \n",
    "        Uc = self.U(c)\n",
    "        \n",
    "        probs = F.log_softmax(Uc, dim=-1)     # log???\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastLayer(\n",
       "  (U): Linear(in_features=128, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRNN_model = CRNN()\n",
    "attn_layer = AttnMech()\n",
    "last_layer = LastLayer()\n",
    "\n",
    "CRNN_model.to(device)\n",
    "attn_layer.to(device)\n",
    "last_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('crnn_red')\n",
    "CRNN_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load('attn_red')\n",
    "attn_layer.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load('last_layer_red')\n",
    "last_layer.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142896"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(CRNN_model) +count_parameters(attn_layer) + count_parameters(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkirili4ik\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">morning-resonance-24</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kirili4ik/dla-ht3\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kirili4ik/dla-ht3/runs/9gi3fi2k\" target=\"_blank\">https://wandb.ai/kirili4ik/dla-ht3/runs/9gi3fi2k</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201103_215216-9gi3fi2k</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f79e5925a50>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.watch(CRNN_model)\n",
    "wandb.watch(attn_layer)\n",
    "wandb.watch(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_crnn = torch.optim.Adam(CRNN_model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "opt_attn = torch.optim.Adam(attn_layer.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "opt_last = torch.optim.Adam(last_layer.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [01:10,  3.20it/s]\n",
      "29it [00:08,  3.46it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [01:08,  3.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9d9fed14fb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mattn_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bb863ca05fe6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mutt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'utt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-082f27fe0df6>\u001b[0m in \u001b[0;36mtransform_tr\u001b[0;34m(wav)\u001b[0m\n\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maugs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maug_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-082f27fe0df6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_rand_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1ea418047b85>\u001b[0m in \u001b[0;36madd_rand_noise\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnoise_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_noises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_noises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnoize_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [0, 40]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_default/lib/python3.7/site-packages/torchaudio/backend/sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msignalinfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mencodinginfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mfiletype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1 epoch\n",
    "NUM_EPOCHS = 35\n",
    "for n in range(NUM_EPOCHS):\n",
    "    CRNN_model.train()\n",
    "    attn_layer.train()\n",
    "    last_layer.train()\n",
    "    for i, el in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        batch = el[0]\n",
    "        labels = el[1]\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "        batch = torch.log(melspec_train(batch) + 1e-9).to(device)   # .batch.narrow(-1, 0, 41)    \n",
    "\n",
    "        opt_crnn.zero_grad()\n",
    "        opt_attn.zero_grad()\n",
    "        opt_last.zero_grad()\n",
    "\n",
    "\n",
    "        hidden = torch.zeros(2*2, batch.size(0), 64).to(device)    # (num_layers*num_dirs,  BS, HS)\n",
    "        e = []\n",
    "\n",
    "        #start = 41\n",
    "        #finish = 81\n",
    "        #for i in range(0, finish - start + 1):\n",
    "        #batch_now = batch[:, :, i : start + i]\n",
    "\n",
    "        batch_now = batch \n",
    "\n",
    "        output, hidden = CRNN_model(batch, hidden)\n",
    "        # output: (seq_len, BS, hidden*num_dir)\n",
    "\n",
    "        for el in output:\n",
    "            e_t = attn_layer(el)       # (BS, 1)        # OR HIDDEN ???\n",
    "            e.append(e_t)\n",
    "\n",
    "        e = torch.cat(e, dim=1)   # (BS, seq_len)\n",
    "\n",
    "        probs = last_layer(e, output)\n",
    "\n",
    "        loss = F.nll_loss(probs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(CRNN_model.parameters(), 10)\n",
    "        torch.nn.utils.clip_grad_norm_(attn_layer.parameters(), 10)\n",
    "        torch.nn.utils.clip_grad_norm_(last_layer.parameters(), 10)\n",
    "\n",
    "        opt_crnn.step()\n",
    "        opt_attn.step()\n",
    "        opt_last.step()\n",
    "\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        acc = torch.true_divide(\n",
    "                            torch.sum(argmax_probs == labels), \n",
    "                            torch.numel(argmax_probs)\n",
    "        )\n",
    "        wandb.log({'loss':loss.item(), 'train_FA':FA, 'train_FR':FR, 'train_acc':acc})\n",
    "        \n",
    "        \n",
    "        \n",
    "    # VALIDATION\n",
    "    CRNN_model.eval()\n",
    "    attn_layer.eval()\n",
    "    last_layer.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        accs = []\n",
    "        FAs, FRs = [], []\n",
    "        for i, el in tqdm(enumerate(val_loader)):\n",
    "            batch = el[0]\n",
    "            labels = el[1]\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "            batch = torch.log(melspec_val(batch) + 1e-9).to(device)  \n",
    "\n",
    "            batch_now = batch \n",
    "\n",
    "            hidden = torch.zeros(2*2, batch.size(0), 64).to(device)    # (num_layers*num_dirs,  BS, HS)\n",
    "            e = []\n",
    "            \n",
    "            output, hidden = CRNN_model(batch, hidden)\n",
    "            # output: (seq_len, BS, hidden*num_dir)\n",
    "\n",
    "            for el in output:\n",
    "                e_t = attn_layer(el)       # (BS, 1)        # OR HIDDEN ???\n",
    "                e.append(e_t)\n",
    "\n",
    "            e = torch.cat(e, dim=1)   # (BS, seq_len)\n",
    "\n",
    "            probs = last_layer(e, output)\n",
    "\n",
    "            loss = F.nll_loss(probs, labels)\n",
    "            \n",
    "            argmax_probs = torch.argmax(probs, dim=-1)\n",
    "            FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            accs.append(torch.true_divide(\n",
    "                                torch.sum(argmax_probs == labels), \n",
    "                                torch.numel(argmax_probs)).item()\n",
    "                       )\n",
    "            FAs.append(FA)\n",
    "            FRs.append(FR)\n",
    "            \n",
    "        wandb.log({'mean_val_loss':np.mean(val_losses), 'mean_val_acc':np.mean(accs),\n",
    "                   'mean_val_FA':np.mean(FAs), 'mean_val_FR':np.mean(FRs)}) \n",
    "    \n",
    "    if n > 0 and n % 10 == 0:\n",
    "        opt_crnn.param_groups[0]['lr'] = 0.0005 * 0.7\n",
    "        opt_attn.param_groups[0]['lr'] = 0.0005 * 0.7\n",
    "        opt_last.param_groups[0]['lr'] = 0.0005 * 0.7\n",
    "\n",
    "    \n",
    "    print('END OF EPOCH', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''torch.save({\n",
    "    'model_state_dict': CRNN_model.state_dict(),\n",
    "}, 'crnn_red')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': attn_layer.state_dict(),\n",
    "}, 'attn_red')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': last_layer.state_dict(),\n",
    "}, 'last_layer_red')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "CRNN_model.eval()\n",
    "attn_layer.eval()\n",
    "last_layer.eval()\n",
    "with torch.no_grad():\n",
    "    val_losses = []\n",
    "    accs = []\n",
    "    FAs, FRs = [], []\n",
    "    for i, el in tqdm(enumerate(val_loader)):\n",
    "        batch = el[0]\n",
    "        labels = el[1]\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "        batch = torch.log(melspec_val(batch) + 1e-9).to(device)  \n",
    "\n",
    "        batch_now = batch \n",
    "\n",
    "        hidden = torch.zeros(2*2, batch.size(0), 64).to(device)    # (num_layers*num_dirs,  BS, HS)\n",
    "        e = []\n",
    "\n",
    "        output, hidden = CRNN_model(batch, hidden)\n",
    "        # output: (seq_len, BS, hidden*num_dir)\n",
    "\n",
    "        for el in output:\n",
    "            e_t = attn_layer(el)       # (BS, 1)        # OR HIDDEN ???\n",
    "            e.append(e_t)\n",
    "\n",
    "        e = torch.cat(e, dim=1)   # (BS, seq_len)\n",
    "\n",
    "        probs = last_layer(e, output)\n",
    "\n",
    "        loss = F.nll_loss(probs, labels)\n",
    "\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "        accs.append(torch.true_divide(\n",
    "                            torch.sum(argmax_probs == labels), \n",
    "                            torch.numel(argmax_probs)).item()\n",
    "                   )\n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "\n",
    "    wandb.log({'mean_val_loss':np.mean(val_losses), 'mean_val_acc':np.mean(accs),\n",
    "               'mean_val_FA':np.mean(FAs), 'mean_val_FR':np.mean(FRs)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ИГРЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 40, 41))   # картинка 40 x 41\n",
    "a = a.unsqueeze(1)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (20, 5)\n",
    "stride = (8, 2)\n",
    "\n",
    "in_size = 1\n",
    "out_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_size, out_size, kernel_size=(20, 5), stride=(8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 3, 19])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(a).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEPARABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 40, 41])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_w = nn.Conv2d(in_size, in_size, kernel_size, \n",
    "                             stride=stride, groups=in_size)\n",
    "point_w = nn.Conv2d(in_size, out_size, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 3, 19])\n",
      "torch.Size([3, 32, 3, 19])\n"
     ]
    }
   ],
   "source": [
    "depth_res = depth_w(a)\n",
    "print(depth_res.size())\n",
    "\n",
    "res = point_w(depth_res)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3232"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(depth_w) + count_parameters(point_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ОНО???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 41])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 40, 41))   # картинка 40 x 41\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бегаем вправо-влево по картинке 40x41 окошком 5x1, таких окошек 40, для каждой строчки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv1d(40, 40, kernel_size=5, groups=40, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40, 19])\n"
     ]
    }
   ],
   "source": [
    "res1 = layer(a)\n",
    "print(res1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бегаем вправо-влево по картинке 40x? окошком 20x1, а 20 берется из 40/groups=20. При этом глубину выставляем 32, это все предыдущее предложение просто проделывается 32 раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = nn.Conv1d(40, 32, kernel_size=1, groups=2, stride=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "res2 = layer2(res1)\n",
    "print(res2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КОНЕЦ ИГР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
